{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_templet = \"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --output=slurm_logs/%x_%A.out\n",
    "#SBATCH --error=slurm_logs/%x_%A.err\n",
    "#SBATCH --partition=C9654\n",
    "#SBATCH --nodelist=c3\n",
    "#SBATCH --ntasks-per-node=1\n",
    "#SBATCH --cpus-per-task=64\n",
    "#SBATCH --mem-per-gpu=100G\n",
    "#SBATCH --gres=gpu:1\n",
    "export PATH=/opt/share/miniconda3/envs/mofmthnn/bin/:$PATH\n",
    "export LD_LIBRARY_PATH=/opt/share/miniconda3/envs/mofmthnn/lib/:$LD_LIBRARY_PATH\n",
    "\n",
    "srun python -u {py_executor} --progress_bar --task_cfg {task_config} --model_cfg {model_config}\n",
    "\"\"\".strip()\n",
    "\n",
    "def run_slurm_job(work_dir, executor=\"sbatch\", script_name=\"run\"):\n",
    "    work_dir = Path(work_dir)\n",
    "    # Create a script to run the job\n",
    "    process = subprocess.Popen(\n",
    "        f\"{executor} {work_dir/script_name}\",\n",
    "        # [executor, str(work_dir/'run'), \"&\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        shell=True,                                                                                                                         \n",
    "        env=os.environ.copy(),\n",
    "        cwd=str(work_dir)\n",
    "    )\n",
    "    return process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training original CGCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 199555\n",
      "Submitted job opt_tsd_ssd_ws24_cgcnn_raw with PID 1736896\n"
     ]
    }
   ],
   "source": [
    "work_dir = Path(\"./CGCNN_MT\").absolute()\n",
    "\n",
    "task_configs = [\n",
    "    # \"tsd_ssd\",\n",
    "    \"tsd_ssd_ws24\",\n",
    "    # \"tsd_ssd_ws24_water\",\n",
    "    # \"tsd_ssd_ws24_water_water4\",\n",
    "    # \"tsd_ssd_ws24_water\",\n",
    "    # \"ssd_ws24\",\n",
    "    # \"ws24\",\n",
    "    \"tsd\",\n",
    "    \"ssd\",\n",
    "    \"ws24_water\",\n",
    "    \"ws24_water4\",\n",
    "    \"ws24_acid\",\n",
    "    \"ws24_base\",\n",
    "    \"ws24_boiling\"\n",
    "                    ]\n",
    "model_configs = [\n",
    "    # \"att_cgcnn\",\n",
    "    # \"cgcnn\",\n",
    "    \"cgcnn_raw\",\n",
    "    # \"fcnn\",\n",
    "    # \"att_fcnn\",\n",
    "    # \"cgcnn_uni_atom\"\n",
    "]\n",
    "script_name = \"run_slurm.sh\"\n",
    "py_executor = \"hyperopt.py\"\n",
    "# py_executor = \"main.py\"\n",
    "model_conf = {\n",
    "            'batch_size': 32,\n",
    "            'max_epochs': 500, \n",
    "            'max_graph_len': 200,\n",
    "            'atom_fea_len': 256,\n",
    "            'extra_fea_len': 16,\n",
    "            'h_fea_len': 128,\n",
    "            'n_conv': 6,\n",
    "            'n_h': 4,\n",
    "            'dropout_prob': 0.5,\n",
    "            'use_extra_fea': False,\n",
    "            'use_cell_params': False,\n",
    "            'atom_layer_norm': False,\n",
    "            'loss_aggregation': \"fixed_weight_sum\",   # fixed_weight_sum, dwa, sum, sample_weight_sum, trainable_weight_sum\n",
    "            'dl_sampler': 'random',\n",
    "            'task_att_type': 'none',\n",
    "            'augment': False,\n",
    "            'lr': 0.001,\n",
    "            'lr_mult': 10,\n",
    "            'group_lr': False,\n",
    "            'optim_config': \"fine\",  # fine or coarse\n",
    "            'auto_lr_bs_find': False, \n",
    "            'patience': 50,\n",
    "            'task_norm': False,\n",
    "            'log_dir': \"logs\",\n",
    "            'optuna_name': \"optuna\",\n",
    "            }\n",
    "\n",
    "for task_config in task_configs:\n",
    "    for model_config in model_configs:\n",
    "        job_name = f\"{task_config.replace('_config', '')}_{model_config.replace('_config', '')}\"\n",
    "        if py_executor == \"hyperopt.py\":\n",
    "            job_name = \"opt_\" + job_name\n",
    "            # job_templet_ = job_templet + \" --pruning\"\n",
    "            job_templet_ = job_templet\n",
    "        else:\n",
    "            job_templet_ = job_templet\n",
    "        job_script = job_templet_.format(job_name=job_name, \n",
    "                                        task_config=task_config, \n",
    "                                        model_config=model_config,\n",
    "                                        py_executor=py_executor\n",
    "                                        )\n",
    "        \n",
    "        for key, value in model_conf.items():\n",
    "            if isinstance(value, bool):\n",
    "                if value:\n",
    "                    job_script += f\" --{key}\"\n",
    "                continue\n",
    "            job_script += f\" --{key} {value}\"\n",
    "        with open(work_dir/script_name, \"w\") as f:\n",
    "            f.write(job_script)\n",
    "        process = run_slurm_job(work_dir, executor=\"sbatch\", script_name=script_name)\n",
    "        ## get the output of the job\n",
    "        while True:\n",
    "            output = process.stdout.readline()\n",
    "            if output == b'' and process.poll() is not None:\n",
    "                break\n",
    "            if output:\n",
    "                print(output.decode().strip())\n",
    "        print(f\"Submitted job {job_name} with PID {process.pid}\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training refined CGCNN model (MOFSNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected GPU: NVIDIA L4 (23.7 GB)\n",
      "Using batch_size: 32\n",
      "\n",
      "============================================================\n",
      "Running MOFSNN Training\n",
      "============================================================\n",
      "Task: tsd_ssd_ws24\n",
      "Model: att_cgcnn\n",
      "Batch Size: 32\n",
      "Num Workers: 8\n",
      "Command:\n",
      "python -u hyperopt.py --progress_bar --task_cfg tsd_ssd_ws24 --model_cfg att_cgcnn --batch_size 32 --max_epochs 500 --max_graph_len 200 --atom_fea_len 256 --extra_fea_len 16 --h_fea_len 128 --n_conv 6 --n_h 4 --dropout_prob 0.5 --use_cell_params --atom_layer_norm --loss_aggregation fixed_weight_sum --dl_sampler random --task_att_type self --lr 0.001 --lr_mult 10 --group_lr --optim_config fine --patience 50 --task_norm --log_dir logs --optuna_name optuna --num_workers 8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dharunkraja/miniconda3/envs/mofsnn/lib/python3.9/site-packages/lightning_fabric/__init__.py:36: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dharunkraja/Desktop/D/MOFSNN_D/CGCNN_MT/hyperopt.py\", line 13, in <module>\n",
      "    from CGCNN_MT.module.module import MInterface\n",
      "  File \"/home/dharunkraja/Desktop/D/MOFSNN_D/CGCNN_MT/module/module.py\", line 26, in <module>\n",
      "    from transformers import (\n",
      "ModuleNotFoundError: No module named 'transformers'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training completed with return code: 1\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def get_optimal_config():\n",
    "    \"\"\"Auto-detect GPU and set optimal batch size\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
    "        \n",
    "        # Optimal batch sizes based on GPU memory\n",
    "        if gpu_mem >= 40:  # A100\n",
    "            batch_size = 128\n",
    "        elif gpu_mem >= 24:  # L4, A10\n",
    "            batch_size = 64\n",
    "        elif gpu_mem >= 16:  # T4, V100\n",
    "            batch_size = 32\n",
    "        else:\n",
    "            batch_size = 16\n",
    "            \n",
    "        print(f\"Detected GPU: {gpu_name} ({gpu_mem:.1f} GB)\")\n",
    "        print(f\"Using batch_size: {batch_size}\")\n",
    "        return batch_size, gpu_mem\n",
    "    else:\n",
    "        print(\"No GPU detected, using CPU\")\n",
    "        return 16, 0\n",
    "\n",
    "def get_num_workers():\n",
    "    \"\"\"Get optimal number of data loading workers\"\"\"\n",
    "    cpu_count = os.cpu_count() or 4\n",
    "    # Use 4 workers per GPU, max half of CPUs\n",
    "    return min(cpu_count // 2, 8)\n",
    "\n",
    "# Auto-detect optimal settings\n",
    "batch_size, gpu_mem = get_optimal_config()\n",
    "num_workers = get_num_workers()\n",
    "\n",
    "work_dir = Path(\"./CGCNN_MT\").absolute()\n",
    "\n",
    "# MOFSNN configuration\n",
    "task_config = \"tsd_ssd_ws24\"\n",
    "model_config = \"att_cgcnn\"\n",
    "\n",
    "model_conf = {\n",
    "    'batch_size': batch_size,  # Auto-scaled\n",
    "    'max_epochs': 500, \n",
    "    'max_graph_len': 200,\n",
    "    'atom_fea_len': 256,\n",
    "    'extra_fea_len': 16,\n",
    "    'h_fea_len': 128,\n",
    "    'n_conv': 6,\n",
    "    'n_h': 4,\n",
    "    'dropout_prob': 0.5,\n",
    "    'use_extra_fea': False,\n",
    "    'use_cell_params': True,\n",
    "    'atom_layer_norm': True,\n",
    "    'loss_aggregation': \"fixed_weight_sum\",\n",
    "    'dl_sampler': 'random',\n",
    "    'task_att_type': 'self',\n",
    "    'augment': False,\n",
    "    'lr': 0.001,\n",
    "    'lr_mult': 10,\n",
    "    'group_lr': True,\n",
    "    'optim_config': \"fine\",\n",
    "    'auto_lr_bs_find': False,\n",
    "    'patience': 50,\n",
    "    'task_norm': True,\n",
    "    'log_dir': \"logs\",\n",
    "    'optuna_name': \"optuna\",\n",
    "    'num_workers': num_workers,  # Add this for data loading\n",
    "}\n",
    "\n",
    "# Build command\n",
    "cmd = f\"python -u hyperopt.py --progress_bar --task_cfg {task_config} --model_cfg {model_config}\"\n",
    "\n",
    "for key, value in model_conf.items():\n",
    "    if isinstance(value, bool):\n",
    "        if value:\n",
    "            cmd += f\" --{key}\"\n",
    "    else:\n",
    "        cmd += f\" --{key} {value}\"\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Running MOFSNN Training\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Task: {task_config}\")\n",
    "print(f\"Model: {model_config}\")\n",
    "print(f\"Batch Size: {batch_size}\")\n",
    "print(f\"Num Workers: {num_workers}\")\n",
    "print(f\"Command:\\n{cmd}\\n\")\n",
    "\n",
    "# Run training\n",
    "process = subprocess.run(\n",
    "    cmd,\n",
    "    shell=True,\n",
    "    cwd=str(work_dir),\n",
    "    env=os.environ.copy()\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining completed with return code: {process.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mofsnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
